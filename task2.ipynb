{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T04:32:55.167804Z",
     "start_time": "2024-05-22T04:32:54.651384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('emails.csv')"
   ],
   "id": "b9d11a20767b681d",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T04:32:55.179531Z",
     "start_time": "2024-05-22T04:32:55.168635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print first few rows\n",
    "print(data.head())\n",
    "\n",
    "# Dimensions & missing values\n",
    "print(f\"\\ndata dimensions: {data.shape}\")\n",
    "print(f\"\\nmissing values for each column:\\n{data.isnull().sum()}\")"
   ],
   "id": "5325ff95a8f18911",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Email No.  the  to  ect  and  for  of    a  you  hou  ...  connevey  jay  \\\n",
      "0   Email 1    0   0    1    0    0   0    2    0    0  ...         0    0   \n",
      "1   Email 2    8  13   24    6    6   2  102    1   27  ...         0    0   \n",
      "2   Email 3    0   0    1    0    0   0    8    0    0  ...         0    0   \n",
      "3   Email 4    0   5   22    0    5   1   51    2   10  ...         0    0   \n",
      "4   Email 5    7   6   17    1    5   2   57    0    9  ...         0    0   \n",
      "\n",
      "   valued  lay  infrastructure  military  allowing  ff  dry  Prediction  \n",
      "0       0    0               0         0         0   0    0           0  \n",
      "1       0    0               0         0         0   1    0           0  \n",
      "2       0    0               0         0         0   0    0           0  \n",
      "3       0    0               0         0         0   0    0           0  \n",
      "4       0    0               0         0         0   1    0           0  \n",
      "\n",
      "[5 rows x 3002 columns]\n",
      "\n",
      "Dataset dimensions: (5172, 3002)\n",
      "\n",
      "Missing values per column:\n",
      "Email No.     0\n",
      "the           0\n",
      "to            0\n",
      "ect           0\n",
      "and           0\n",
      "             ..\n",
      "military      0\n",
      "allowing      0\n",
      "ff            0\n",
      "dry           0\n",
      "Prediction    0\n",
      "Length: 3002, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T04:32:55.217759Z",
     "start_time": "2024-05-22T04:32:55.180307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = data.iloc[:,1:-1]  # all columns except first (id) and last (label)\n",
    "y = data.iloc[:,-1]    # last column is label"
   ],
   "id": "2eba7732b89d62cd",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T04:32:55.283666Z",
     "start_time": "2024-05-22T04:32:55.218828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=420)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=69) # funny numbers\n",
    "\n",
    "# 60/20/20 split for train/validation/test"
   ],
   "id": "909747316e692d73",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T04:32:55.393597Z",
     "start_time": "2024-05-22T04:32:55.284341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  \n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ],
   "id": "83866c95b5f21af7",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T04:32:56.047201Z",
     "start_time": "2024-05-22T04:32:55.394310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# trying logistic regression first\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# train\n",
    "lr = LogisticRegression(random_state=10)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# do predictions on validation set\n",
    "lr_pred = lr.predict(X_val_scaled)\n",
    " \n",
    "print(\"LR performance on validation set:\")\n",
    "print(\"Accuracy:  \", accuracy_score(y_val, lr_pred))\n",
    "print(\"Precision: \", precision_score(y_val, lr_pred))\n",
    "print(\"Recall:    \", recall_score(y_val, lr_pred))\n",
    "print(\"F1 Score:  \", f1_score(y_val, lr_pred))"
   ],
   "id": "962d5698c5ac2a34",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR performance on validation set:\n",
      "Accuracy:   0.9698067632850241\n",
      "Precision:  0.9291666666666667\n",
      "Recall:     0.9653679653679653\n",
      "F1 Score:   0.9469214437367304\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T04:33:55.515158Z",
     "start_time": "2024-05-22T04:33:48.703765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# same thing but with an SVM instead\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(random_state=11)\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "svm_pred = svm.predict(X_val_scaled)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"\\nSVM Performance on validation set:\")\n",
    "print(\"Accuracy:  \", accuracy_score(y_val, svm_pred))\n",
    "print(\"Precision: \", precision_score(y_val, svm_pred))\n",
    "print(\"Recall:    \", recall_score(y_val, svm_pred))\n",
    "print(\"F1 Score:  \", f1_score(y_val, svm_pred))\n"
   ],
   "id": "e7d3a84567728991",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM Performance on validation set:\n",
      "Accuracy:   0.927536231884058\n",
      "Precision:  0.9523809523809523\n",
      "Recall:     0.7792207792207793\n",
      "F1 Score:   0.8571428571428571\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The logistic regression model achieved an accuracy of about 97% on the validation set, with high precision (93%), recall (97%), and F1 score (95%). This (as far as I can tell) means the model is doing a good job of identifying spam emails but also not missing too many spam emails (high recall valyue).\n",
    "\n",
    "SVM model also performed well, with an accuracy of about 93%. However its recall is lower (78%), resulting in a lower F1 score (86%) when it does classify an email as spam, it's missing quite a few spam emails."
   ],
   "id": "58b973c84091a21f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T04:38:34.138138Z",
     "start_time": "2024-05-22T04:38:30.302286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# hyperparameter tuning\n",
    "# I have no idea wtf I'm doing... I'm just copy-pasting from Kaggle for the most part and just adapting for this dataset. ðŸ’€ðŸ’€ðŸ’€\n",
    "# https://www.kaggle.com/code/sfktrkl/titanic-hyperparameter-tuning-gridsearchcv\n",
    "# https://www.kaggle.com/code/aravindnaidu/ensemble-hyperparams-tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# parameter grid\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'penalty': ['l1', 'l2']}\n",
    "\n",
    "# perform grid search\n",
    "grid_search = GridSearchCV(estimator=LogisticRegression(random_state=42069), \n",
    "                           param_grid=param_grid, \n",
    "                           cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)"
   ],
   "id": "89c7ebb20c0f181d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jai/PycharmProjects/HIT391/.venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:547: FitFailedWarning: \n",
      "20 fits failed out of a total of 40.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jai/PycharmProjects/HIT391/.venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/jai/PycharmProjects/HIT391/.venv/lib/python3.12/site-packages/sklearn/base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jai/PycharmProjects/HIT391/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jai/PycharmProjects/HIT391/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/jai/PycharmProjects/HIT391/.venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [       nan 0.97340704        nan 0.96494554        nan 0.95617827\n",
      "        nan 0.96161817]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'C': 0.1, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T04:42:15.844387Z",
     "start_time": "2024-05-22T04:42:13.728698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# hyperparameter tuning without the 'l1' lasso regularisation solver failed because it doesn't support it?\n",
    "# I dunno, just removed it and now it doesn't throw any error lol. \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# parameter grid\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'penalty': ['l2']}\n",
    "\n",
    "# perform grid search\n",
    "grid_search = GridSearchCV(estimator=LogisticRegression(random_state=42069), \n",
    "                           param_grid=param_grid, \n",
    "                           cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)"
   ],
   "id": "2378e63f7f7dd826",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Best parameters found:  {'C': 0.1, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T04:46:17.703799Z",
     "start_time": "2024-05-22T04:46:17.668678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# evaluating performance on validation set again\n",
    "best_lr = grid_search.best_estimator_\n",
    "lr_pred = best_lr.predict(X_val_scaled)\n",
    "\n",
    "print(\"\\nBest LR performance on validation set:\")\n",
    "print(\"Accuracy:  \", accuracy_score(y_val, lr_pred))\n",
    "print(\"Precision: \", precision_score(y_val, lr_pred))\n",
    "print(\"Recall:    \", recall_score(y_val, lr_pred))\n",
    "print(\"F1 Score:  \", f1_score(y_val, lr_pred))\n"
   ],
   "id": "1b906cd6894d70a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best LR performance on validation set:\n",
      "Accuracy:   0.9782608695652174\n",
      "Precision:  0.9493670886075949\n",
      "Recall:     0.974025974025974\n",
      "F1 Score:   0.9615384615384616\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T04:47:05.683160Z",
     "start_time": "2024-05-22T04:47:05.648896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Looks good? Now for the test set:\n",
    "lr_pred_test = best_lr.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\nBest LR performance on test set:\")\n",
    "print(\"Accuracy:  \", accuracy_score(y_test, lr_pred_test))\n",
    "print(\"Precision: \", precision_score(y_test, lr_pred_test))\n",
    "print(\"Recall:    \", recall_score(y_test, lr_pred_test))\n",
    "print(\"F1 Score:  \", f1_score(y_test, lr_pred_test))"
   ],
   "id": "9fbbe2d9eb9cbbcf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best LR performance on test set:\n",
      "Accuracy:   0.9758454106280193\n",
      "Precision:  0.9543859649122807\n",
      "Recall:     0.9577464788732394\n",
      "F1 Score:   0.9560632688927944\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# TL;DR\n",
    "\n",
    "Tuned logistic regression: \n",
    "- Accuracy: 97.8%\n",
    "- Precision: 94.9%\n",
    "- Recall: 97.4%\n",
    "- F1 Score: 96.2%\n",
    "\n",
    "Improvement over the initial logistic regression model that has an accuracy of 97.0% and an F1 score of 94.7%.\n",
    "\n",
    "Model performed similarly well on the test set:\n",
    "- Accuracy: 97.6%\n",
    "- Precision: 95.4%\n",
    "- Recall: 95.8%\n",
    "- F1 Score: 95.6%\n",
    "\n",
    "**Good enough**"
   ],
   "id": "2f45957831013fff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cb293b5aa07391bc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
